/*
 *
 * Copyright 2021 gRPC authors.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 *     http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 *
 */

package clusterresolver

import (
	"encoding/json"
	"fmt"
	"sort"

	"google.golang.org/grpc/balancer/roundrobin"
	"google.golang.org/grpc/balancer/weightedroundrobin"
	"google.golang.org/grpc/balancer/weightedtarget"
	"google.golang.org/grpc/internal/hierarchy"
	internalserviceconfig "google.golang.org/grpc/internal/serviceconfig"
	"google.golang.org/grpc/resolver"
	"google.golang.org/grpc/xds/internal"
	"google.golang.org/grpc/xds/internal/balancer/clusterimpl"
	"google.golang.org/grpc/xds/internal/balancer/priority"
	"google.golang.org/grpc/xds/internal/balancer/ringhash"
	"google.golang.org/grpc/xds/internal/xdsclient/xdsresource"
)

const million = 1000000

// priorityConfig is config for one priority. For example, if there an EDS and a
// DNS, the priority list will be [priorityConfig{EDS}, priorityConfig{DNS}].
//
// Each priorityConfig corresponds to one discovery mechanism from the LBConfig
// generated by the CDS balancer. The CDS balancer resolves the cluster name to
// an ordered list of discovery mechanisms (if the top cluster is an aggregated
// cluster), one for each underlying cluster.
type priorityConfig struct {
	mechanism DiscoveryMechanism
	// edsResp is set only if type is EDS.
	edsResp xdsresource.EndpointsUpdate
	// addresses is set only if type is DNS.
	addresses []string
	// Each discovery mechanism has a name generator so that the child policies
	// can reuse names between updates (EDS updates for example).
	childNameGen *nameGenerator
}

// buildPriorityConfigJSON builds balancer config for the passed in
// priorities.
//
// The built tree of balancers (see test for the output struct).
//
// If xds lb policy is ROUND_ROBIN, the children will be weighted_target for
// locality picking, and round_robin for endpoint picking.
//
//                                   ┌────────┐
//                                   │priority│
//                                   └┬──────┬┘
//                                    │      │
//                        ┌───────────▼┐    ┌▼───────────┐
//                        │cluster_impl│    │cluster_impl│
//                        └─┬──────────┘    └──────────┬─┘
//                          │                          │
//           ┌──────────────▼─┐                      ┌─▼──────────────┐
//           │locality_picking│                      │locality_picking│
//           └┬──────────────┬┘                      └┬──────────────┬┘
//            │              │                        │              │
//          ┌─▼─┐          ┌─▼─┐                    ┌─▼─┐          ┌─▼─┐
//          │LRS│          │LRS│                    │LRS│          │LRS│
//          └─┬─┘          └─┬─┘                    └─┬─┘          └─┬─┘
//            │              │                        │              │
// ┌──────────▼─────┐  ┌─────▼──────────┐  ┌──────────▼─────┐  ┌─────▼──────────┐
// │endpoint_picking│  │endpoint_picking│  │endpoint_picking│  │endpoint_picking│
// └────────────────┘  └────────────────┘  └────────────────┘  └────────────────┘
//
// If xds lb policy is RING_HASH, the children will be just a ring_hash policy.
// The endpoints from all localities will be flattened to one addresses list,
// and the ring_hash policy will pick endpoints from it.
//
//           ┌────────┐
//           │priority│
//           └┬──────┬┘
//            │      │
// ┌──────────▼─┐  ┌─▼──────────┐
// │cluster_impl│  │cluster_impl│
// └──────┬─────┘  └─────┬──────┘
//        │              │
// ┌──────▼─────┐  ┌─────▼──────┐
// │ ring_hash  │  │ ring_hash  │
// └────────────┘  └────────────┘
//
// If endpointPickingPolicy is nil, roundrobin will be used.
//
// Custom locality picking policy isn't support, and weighted_target is always
// used.
func buildPriorityConfigJSON(priorities []priorityConfig, xdsLBPolicy *internalserviceconfig.BalancerConfig) ([]byte, []resolver.Address, /*locality weight map, */ error) {
	pc, addrs, err := buildPriorityConfig(priorities, xdsLBPolicy)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to build priority config: %v", err)
	}
	ret, err := json.Marshal(pc)
	if err != nil {
		return nil, nil, fmt.Errorf("failed to marshal built priority config struct into json: %v", err)
	}
	return ret, addrs, nil
}

func buildPriorityConfig(priorities []priorityConfig, xdsLBPolicy *internalserviceconfig.BalancerConfig) (*priority.LBConfig, []resolver.Address, error) {
	var (
		retConfig = &priority.LBConfig{Children: make(map[string]*priority.Child)}
		retAddrs  []resolver.Address
	)
	for _, p := range priorities {
		switch p.mechanism.Type {
		case DiscoveryMechanismTypeEDS:
			// cluster impl child with either weighted target or ring as child policy

			// now all you have to do is take this xdsLBPolicy serviceconfig.LoadBalancingConfig
			// and put it on as the child for each priority
			// from all EDS children wtf

			// addrs is constant, I think only thing that changes is config,
			// unconditionally just put it there xDS WRR Locality uses as needed others just ignore, I think you
			// keep all the same validations and stuff for EDS and you keep
			// everything else as such

			// configs here change, addrs don't

			names, configs, addrs, err := buildClusterImplConfigForEDS(p.childNameGen, p.edsResp, p.mechanism, xdsLBPolicy)
			if err != nil {
				return nil, nil, err
			}
			retConfig.Priorities = append(retConfig.Priorities, names...)
			for n, c := range configs {
				retConfig.Children[n] = &priority.Child{
					Config: &internalserviceconfig.BalancerConfig{Name: clusterimpl.Name, Config: c},
					// Ignore all re-resolution from EDS children.
					IgnoreReresolutionRequests: true,
				}
			}
			retAddrs = append(retAddrs, addrs...)
		// Is this flow vvv kept the same??? Yeah, I think every
		// node in the tree that is LogicalDNS is cluster impl with pick first

		// Every node in EDS (ring hash, XDS WRR which is what replaces RR custom LB, you just pass as a opaque blob
		// in what you receive from the xdsclient),

		// cluster impl itself handles config



		case DiscoveryMechanismTypeLogicalDNS: // this is kept the same, child as pick first
			// cluster impl with child as pick first, all endpoints in a single
			// priority and locality

			// I think this config, and addrs are still built here

			// This flow stays the same

			name, config, addrs := buildClusterImplConfigForDNS(p.childNameGen, p.addresses, p.mechanism) // why desn't this take xDSLBPolicy? Does Logical DNS End the balancer tree?
			retConfig.Priorities = append(retConfig.Priorities, name)
			retConfig.Children[name] = &priority.Child{
				Config: &internalserviceconfig.BalancerConfig{Name: clusterimpl.Name, Config: config},
				// Not ignore re-resolution from DNS children, they will trigger
				// DNS to re-resolve.
				IgnoreReresolutionRequests: false,
			}
			retAddrs = append(retAddrs, addrs...)
		}
	}
	return retConfig, retAddrs, nil
}

func buildClusterImplConfigForDNS(g *nameGenerator, addrStrs []string, mechanism DiscoveryMechanism) (string, *clusterimpl.LBConfig, []resolver.Address) {
	// Endpoint picking policy for DNS is hardcoded to pick_first.

	// Doesn't care about locality weights or endpoint weights...

	const childPolicy = "pick_first"
	retAddrs := make([]resolver.Address, 0, len(addrStrs))
	pName := fmt.Sprintf("priority-%v", g.prefix)
	for _, addrStr := range addrStrs {
		retAddrs = append(retAddrs, hierarchy.Set(resolver.Address{Addr: addrStr}, []string{pName}))
	}
	return pName, &clusterimpl.LBConfig{
		Cluster:     mechanism.Cluster,
		ChildPolicy: &internalserviceconfig.BalancerConfig{Name: childPolicy},
	}, retAddrs
}

// buildClusterImplConfigForEDS returns a list of cluster_impl configs, one for
// each priority, sorted by priority, and the addresses for each priority (with
// hierarchy attributes set).
//
// For example, if there are two priorities, the returned values will be
// - ["p0", "p1"]
// - map{"p0":p0_config, "p1":p1_config}
// - [p0_address_0, p0_address_1, p1_address_0, p1_address_1]
//   - p0 addresses' hierarchy attributes are set to p0
func buildClusterImplConfigForEDS(g *nameGenerator, edsResp xdsresource.EndpointsUpdate, mechanism DiscoveryMechanism, xdsLBPolicy *internalserviceconfig.BalancerConfig) ([]string, map[string]*clusterimpl.LBConfig, []resolver.Address, error) {
	drops := make([]clusterimpl.DropConfig, 0, len(edsResp.Drops))
	for _, d := range edsResp.Drops {
		drops = append(drops, clusterimpl.DropConfig{
			Category:           d.Category,
			RequestsPerMillion: d.Numerator * million / d.Denominator,
		})
	}
	// To provide the xds_wrr_locality load balancer information about locality
	// weights received from EDS, the cluster resolver will populate a new
	// attribute in the resolved addresses. The new attribute will contain a map
	// from a locality struct to a locality weight integer. Note that a change
	// in this attribute would still allow associated subchannels to be reused -
	// it will not affect their uniqueness.
	/*
	type Locality struct {
	    Endpoints []Endpoint
	    ID        internal.LocalityID
	    Priority  uint32
	    Weight    uint32
	}
	*/
	priorities := groupLocalitiesByPriority(edsResp.Localities)
	retNames := g.generate(priorities)
	retConfigs := make(map[string]*clusterimpl.LBConfig, len(retNames))
	var retAddrs []resolver.Address
	for i, pName := range retNames {
		priorityLocalities := priorities[i]
		cfg, addrs, err := priorityLocalitiesToClusterImpl(priorityLocalities, pName, mechanism, drops, xdsLBPolicy)
		if err != nil {
			return nil, nil, nil, err
		}
		retConfigs[pName] = cfg
		retAddrs = append(retAddrs, addrs...)
	}
	return retNames, retConfigs, retAddrs, nil
}

// groupLocalitiesByPriority returns the localities grouped by priority.
//
// The returned list is sorted from higher priority to lower. Each item in the
// list is a group of localities.
//
// For example, for L0-p0, L1-p0, L2-p1, results will be
// - [[L0, L1], [L2]]
func groupLocalitiesByPriority(localities []xdsresource.Locality) [][]xdsresource.Locality {
	var priorityIntSlice []int
	priorities := make(map[int][]xdsresource.Locality)
	for _, locality := range localities {
		if locality.Weight == 0 {
			continue
		}
		priority := int(locality.Priority)
		priorities[priority] = append(priorities[priority], locality)
		priorityIntSlice = append(priorityIntSlice, priority)
	}
	// Sort the priorities based on the int value, deduplicate, and then turn
	// the sorted list into a string list. This will be child names, in priority
	// order.
	sort.Ints(priorityIntSlice)
	priorityIntSliceDeduped := dedupSortedIntSlice(priorityIntSlice)
	ret := make([][]xdsresource.Locality, 0, len(priorityIntSliceDeduped))
	for _, p := range priorityIntSliceDeduped {
		ret = append(ret, priorities[p])
	}
	return ret
}

func dedupSortedIntSlice(a []int) []int {
	if len(a) == 0 {
		return a
	}
	i, j := 0, 1
	for ; j < len(a); j++ {
		if a[i] == a[j] {
			continue
		}
		i++
		if i != j {
			a[i] = a[j]
		}
	}
	return a[:i+1]
}

// rrBalancerConfig is a const roundrobin config, used as child of
// weighted-roundrobin. To avoid allocating memory everytime.
var rrBalancerConfig = &internalserviceconfig.BalancerConfig{Name: roundrobin.Name}

// priorityLocalitiesToClusterImpl takes a list of localities (with the same
// priority), and generates a cluster impl policy config, and a list of
// addresses.
func priorityLocalitiesToClusterImpl(localities []xdsresource.Locality, priorityName string, mechanism DiscoveryMechanism, drops []clusterimpl.DropConfig, xdsLBPolicy *internalserviceconfig.BalancerConfig) (*clusterimpl.LBConfig, []resolver.Address, error) {
	clusterImplCfg := &clusterimpl.LBConfig{
		Cluster:               mechanism.Cluster,
		EDSServiceName:        mechanism.EDSServiceName,
		LoadReportingServer:   mechanism.LoadReportingServer,
		MaxConcurrentRequests: mechanism.MaxConcurrentRequests,
		DropCategories:        drops,

		// xdsLBPolicy now comes in as JSON and you literally just stick it right here, regardless of the control plane proto configuration it is sent from

		// xdsWRRPolicy will prepare the weighted target + endpoint picking policy, this will stick locality information received from EDS in attributes passed downward
		// to help this policy prepare weighted target
	}

	// this branching logic for config preparation in the xdsclient when the load balancing field isn't set (that will then determine the JSON)
	// gets all of this too.

	// the invariant that determines this (switch on that enum, needs to do this preparation in the xdsclient + marshal into raw JSON)

	// each cluster impl needs this child of xDS LB Policy.

	// this is shared amongst the whole thing

	// what does this default to?


	// this codeblock you change to just putting it on the wire

	// I think these logical invariants are equivalent to
	// the emission of xds_wrr_locality *triage*?
	// and so you can just stick it in for the priority child here. What state of the system causes this function vvv to be called?

	if xdsLBPolicy == nil || xdsLBPolicy.Name == rrName { // rr branch
		// If lb policy is ROUND_ROBIN:
		// - locality-picking policy is weighted_target
		// - endpoint-picking policy is round_robin

		// When do you populate the address with locality weights? You only need
		// it for this weighted round robin thing right?

		logger.Infof("xds lb policy is %q, building config with weighted_target + round_robin", rrName)
		// Child of weighted_target is hardcoded to round_robin.

		// this is the only thing you need to change. I think you get rid of
		// config ... really though what triggers this function to hit

		// just stick child config on there directly, don't need this wtConfig thing

		wtConfig, addrs := localitiesToWeightedTarget(localities, priorityName, rrBalancerConfig)
		clusterImplCfg.ChildPolicy = &internalserviceconfig.BalancerConfig{Name: weightedtarget.Name, Config: wtConfig}
		return clusterImplCfg, addrs, nil
	}

	// With the xDS client now creating a configuration that represents the
	// whole policy hierarchy, this special logic can be removed and the policy
	// selection received can be directly passed to the priority policy.

	// but there's other considerations here outside of the actual config received/built.
	// that builds out hierarchy addresses when it receives a config
	// that corresponds to building out weighted round robin + round robin for endpoints.

	// Now you receive an arbitrary config from xdsclient. What do you do in this scenario?
	// Before only receives xds wrr locality, now can receive anything. What do you do about hierarchical address list?


	// Such as the addresses being built out,


	// If the policy it receives is ring_hash, it will pass that policy selection directly to the priority policy.
	if xdsLBPolicy.Name == rhName { // ring hash branch - I still think you keep this logic you need the logic, also this just takes it and puts it on
		// If lb policy is RING_HASH, will build one ring_hash policy as child.
		// The endpoints from all localities will be flattened to one addresses
		// list, and the ring_hash policy will pick endpoints from it. // This concept of flattening endpoints from all localities to a single address list is how it will work if not configured with wrr locality
		logger.Infof("xds lb policy is %q, building config with ring_hash", rhName)
		addrs := localitiesToRingHash(localities, priorityName)
		// Set child to ring_hash, note that the ring_hash config is from
		// xdsLBPolicy.
		clusterImplCfg.ChildPolicy = &internalserviceconfig.BalancerConfig{Name: ringhash.Name, Config: xdsLBPolicy.Config}
		return clusterImplCfg, addrs, nil
	}

	return nil, nil, fmt.Errorf("unsupported xds LB policy %q, not one of {%q,%q}", xdsLBPolicy.Name, rrName, rhName)
}

// to construct priority config only - not the address logic, how is xdsclient plumbed into addressees?

// localitiesToRingHash takes a list of localities (with the same priority), and
// generates a list of addresses.
//
// The addresses have path hierarchy set to [priority-name], so priority knows
// which child policy they are for.
func localitiesToRingHash(localities []xdsresource.Locality, priorityName string) []resolver.Address {
	var addrs []resolver.Address
	for _, locality := range localities {
		var lw uint32 = 1
		if locality.Weight != 0 {
			lw = locality.Weight
		}
		// actually cares about localities and endpoint weights - this is already handled though.
		localityStr, err := locality.ID.ToString()
		if err != nil {
			localityStr = fmt.Sprintf("%+v", locality.ID)
		}
		for _, endpoint := range locality.Endpoints {
			// Filter out all "unhealthy" endpoints (unknown and healthy are
			// both considered to be healthy:
			// https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/core/health_check.proto#envoy-api-enum-core-healthstatus).
			if endpoint.HealthStatus != xdsresource.EndpointHealthStatusHealthy && endpoint.HealthStatus != xdsresource.EndpointHealthStatusUnknown {
				continue
			}

			var ew uint32 = 1
			if endpoint.Weight != 0 {
				ew = endpoint.Weight
			}

			// The weight of each endpoint is locality_weight * endpoint_weight.
			ai := weightedroundrobin.AddrInfo{Weight: lw * ew}
			addr := weightedroundrobin.SetAddrInfo(resolver.Address{Addr: endpoint.Address}, ai)
			addr = hierarchy.Set(addr, []string{priorityName, localityStr})
			addr = internal.SetLocalityID(addr, locality.ID)
			addrs = append(addrs, addr)
		}
	}
	return addrs
}

// localitiesToWeightedTarget takes a list of localities (with the same
// priority), and generates a weighted target config, and list of addresses.
//
// The addresses have path hierarchy set to [priority-name, locality-name], so
// priority and weighted target know which child policy they are for.

type localityWeightsKeyType string

const localityWeightsKey = localityWeightsKeyType("grpc.xds.internal.balancer.clusterresolver.LocalityWeights")

// FromResolverState returns the Client from state, or nil if not present.
func FromResolverState(state resolver.State) map[string]uint32 { // similar to this, only used in the tree as needed
	cs, _ := state.Attributes.Value(localityWeightsKey).(map[string]uint32)
	return cs
}

// SetClient sets lw in state and returns the new state. (do we want this map to be a pointer?)
func SetLocalityWeights(state resolver.State, lw map[string]uint32) resolver.State { // do we even need this to be exported?
	state.Attributes = state.Attributes.WithValue(localityWeightsKey, lw)
	return state
} // In what sceanrios does this get called


// DOESNT PREPARE A CONFIG ANYMORE, NOW THIS NEEDS TO PREPARE A MAP
// pass through for WRR Locality, I guess child should be passed in
// to determine what you put into addresses
func localitiesToWeightedTarget(localities []xdsresource.Locality, priorityName string, childPolicy *internalserviceconfig.BalancerConfig) (*weightedtarget.LBConfig, []resolver.Address) { // right, two things, the actual LB Configuration, and the address list with address specific information
	var localityWeights map[string]uint32

	// need helpers to put these weights into resolver state,
	// and also


	weightedTargets := make(map[string]weightedtarget.Target)
	var addrs []resolver.Address

	// this has the each locality
	//                  each endpoint iteration

	for _, locality := range localities {
		localityStr, err := locality.ID.ToString()
		if err != nil {
			localityStr = fmt.Sprintf("%+v", locality.ID)
		}

		// this str: weight gets populated in addresses attributes, each addr in a localityyyyy...what's the granularity?
		// weighted target has same configuration

		// the new policy is handling ALL the localities in a single priority.
		// IS it per address or for all of them?

		// Are we populating the attributes in resolver.State or per a list of addresses the attributes in resolver.State - passed in a similar manner to the xdsclient

		weightedTargets[localityStr] = weightedtarget.Target{Weight: locality.Weight, ChildPolicy: childPolicy} // here's the locality weights

		// switch to

		localityWeights[localityStr] = locality.Weight




		// somewhere in this file, switch localityStr: Weight to populate a full map
		// fill out weights unconditionally...?

		// fill out endpoint weights when? when endpoint picking is RR or just
		// in general? Endpoint picking policy...when does it need the number representing locality weight?

		// I'm sure it's useful regardless. Balancers can just ignore otherwise

		// CustomLB will have access to these endpoint weights. might be useful

		// best to set in too many scenarios. it's an attribute so you can just ignore

		// ^^^ unconditionally just put on downward flow (or only when it's a WRR child?) custom LB might want locality weights,
		// and endpoints as well.

		// this stuff is the stuff that moves down to WRR Locality LB Policy



		// Does top level round robin require the locality weights?
		// The endpoints

		for _, endpoint := range locality.Endpoints {
			// Filter out all "unhealthy" endpoints (unknown and healthy are
			// both considered to be healthy:
			// https://www.envoyproxy.io/docs/envoy/latest/api-v2/api/v2/core/health_check.proto#envoy-api-enum-core-healthstatus).
			if endpoint.HealthStatus != xdsresource.EndpointHealthStatusHealthy && endpoint.HealthStatus != xdsresource.EndpointHealthStatusUnknown {
				continue
			}

			// This sets endpoint weights. When does this need to get called?
			// This endpoint weights

			// weighted target + endpoint picking require both numbers.
			// however, when do you populate.

			// seems useful if you can have any child endpoint picking policy of the XDS_WRR_LOCALITY LB...
			// even if it's not WRR...
			// or I guess the Addr Info struct is specific to the weightedroundrobin package, so you might need it.

			addr := resolver.Address{Addr: endpoint.Address} // construction of an address list for healthy endpoints
			if childPolicy.Name == weightedroundrobin.Name && endpoint.Weight != 0 { // wtf, this is incorrect. this literally never hits.
				ai := weightedroundrobin.AddrInfo{Weight: endpoint.Weight} // yeah so do we want to ever populate endpoint weights
				addr = weightedroundrobin.SetAddrInfo(addr, ai)
			}

			// This is a noop regardless - unless there is a balancer that can
			// be weighted round robin for endpoint picking.
			// So just keep this check for the endpoint picking policy that
			// WRR Locality LB Policy puts as child of locality trees.
			// weighted round robin? Other than that all this address information is needed

			// other than that keep these vvv, keep filtering out endpoints ^^^




			// tells the path ish of the address so I still this both of these
			addr = hierarchy.Set(addr, []string{priorityName, localityStr})
			// tells what locality the address is a part of
			addr = internal.SetLocalityID(addr, locality.ID)

			addrs = append(addrs, addr)
		}
	}
	// This logic all gets moved to wrr experimental, which is now the place where
	// the preparation of this config happens...

	// this special logic (i.e. the creation of the child policy, it forwarded ring hash
	// but would create weighted target with endpoint picking policy selection as child, creating two level hierachy.

	// the creation of the weighted target config gets now delegated to xDS WRR Locality LB Policy
	// which does this logic itself with attributes inside the addresses? ClientConnState?
	// and prepares this weighted target.

	// this logic changes, the policy section received (internalserviceconfig.BalancerConfig)
	// can be passed directly down priority policy

	// map from a locality struct to a locality weight integer

	// a change in this attribute would allow associated subchannels to be
	// reused - it will not affect their uniqueness. Thus, it seems to be in a
	// balancer Attribute?

	return &weightedtarget.LBConfig{Targets: weightedTargets}, addrs
}
